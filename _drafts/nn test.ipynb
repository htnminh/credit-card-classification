{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nn test.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1iiIPG32VP_RBcVRpzhWqhWMO7OWc9DxF","authorship_tag":"ABX9TyP3WiJR2OKVJCwMpq4H6pye"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af\n","\n","Following are the thumb-rules for building an MLP. However, most of them are applicable on other Deep Learning models.\n","\n","Number of Layers: Start with two hidden layers (this does not include the last layer).\n","\n","Number of nodes (size) of intermediate layers: a number from the geometric progression of 2, e.g., 4, 8, 16, 32, … . The first layer should be around half of the number of input data features. The next layer size as half of the previous.\n","\n","Number of nodes (size) of output layer for Classification: If binary classification then the size is one. For a multi-class classifier, the size is the number of classes.\n","Size of output layer for regression: If single response then the size one. For multi-response regression, the size is the number of responses.\n","\n","Activation for intermediate layers: Use relu activation.\n","\n","Activation for output layer: Use sigmoid for binary classification, softmax for multi-class classifier, and linear for regression. For Autoencoders, the last layer should be linear if the input data is continuous, otherwise, sigmoid or softmax for binary or multi-level categorical input.\n","\n","Dropout layers: Add Dropout after every layer, except the Input layer (if defining the Input layer separately). Set Dropout rate to 0.5. Dropout rate > 0.5 is counter-productive. If you believe a rate of 0.5 is regularizing too many nodes, then increase the size of the layer instead of reducing the Dropout rate to less than 0.5. I prefer to not set any Dropout on the Input layer. But if you feel compelled to do that, set the Dropout rate < 0.2.\n","\n","Data preprocessing: I am assuming your predictors X is numeric and you have already converted any categorical columns into one-hot-encoding. Before using the data for model training, perform data scaling. UseMinMaxScaler from sklearn.preprocessing. If this does not work well, do StandardScaler present in the same library. The scaling is needed for y in regression.\n","\n","Split data to train, valid, test: Use train_test_split from sklearn.model_selection. See example below.\n","\n","Class weights: If you have unbalanced data, then set class weights to balance the loss in your model.fit . For a binary classifier, the weights should be: {0: number of 1s / data size, 1: number of 0s / data size}. For extremely unbalanced data (rare events), class weight may not work. Be cautious adding it.\n","\n","Optimizer: Use adam with its default learning rate.\n","\n","Loss in classification: For binary classification use binary_crossentropy. For multiclass, use categorical_crossentropy if the labels are one-hot-encoded, otherwise use sparse_categorical_crossentropy if the labels are integers.\n","\n","Epochs: Start with 20 to see if the model training shows decreasing loss and any improvement in accuracy. If there is no minimal success with 20 epochs, move on. If you get some minimal success, make epoch as 100.\n","\n","Batch size: Choose the batch size from the geometric progression of 2. For unbalanced datasets have larger value, like 128, otherwise start with 16.\n"],"metadata":{"id":"lemQKngdrFvK"}},{"cell_type":"markdown","source":["tried the solutions here but none of them work\n","\n","https://stackoverflow.com/questions/58479556/notimplementederror-cannot-convert-a-symbolic-tensor-2nd-target0-to-a-numpy\n","\n","https://stackoverflow.com/questions/66775948/downgrade-python-version-from-3-7-to-3-6-in-google-colab\n","\n","it is still an open issue on tensorflow github\n","\n","https://github.com/tensorflow/models/issues/9706"],"metadata":{"id":"ipiXCRixU95W"}},{"cell_type":"code","source":["# ! pip install -U tensorflow numpy --quiet"],"metadata":{"id":"5w3TalOQUQWN","executionInfo":{"status":"ok","timestamp":1658338021967,"user_tz":-420,"elapsed":14,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"vI7XyDOEd020","executionInfo":{"status":"ok","timestamp":1658338021967,"user_tz":-420,"elapsed":13,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# %%bash\n","\n","# MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n","# MINICONDA_PREFIX=/usr/local\n","# wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n","# chmod +x $MINICONDA_INSTALLER_SCRIPT\n","# ./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"],"metadata":{"id":"BZ0bOw8Sdgvu","executionInfo":{"status":"ok","timestamp":1658338021968,"user_tz":-420,"elapsed":13,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# import sys\n","# _ = (sys.path.append(\"/usr/local/lib/python3.6/site-packages\"))"],"metadata":{"id":"2t5f_HDZdnfT","executionInfo":{"status":"ok","timestamp":1658338021968,"user_tz":-420,"elapsed":12,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"kN_h-Niwr4Og","executionInfo":{"status":"ok","timestamp":1658338025495,"user_tz":-420,"elapsed":3539,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}}},"outputs":[],"source":["import os\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras import models\n","from tensorflow.keras import layers\n","\n","from sklearn.metrics import fbeta_score\n","from sklearn.metrics import make_scorer\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6kf8Ols9axlp","executionInfo":{"status":"ok","timestamp":1658338025495,"user_tz":-420,"elapsed":4,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}}},"outputs":[],"source":["DIR_PATH = \"/content/drive/MyDrive/Colab Notebooks/ML CCC\"\n","\n","RANDOM_STATE = 42"]},{"cell_type":"code","source":["f2_score = lambda y_test, y_pred: fbeta_score(y_test, y_pred, beta=2)\n","f2_scorer = make_scorer(fbeta_score, beta=2)"],"metadata":{"id":"njopzynfgpnh","executionInfo":{"status":"ok","timestamp":1658338025496,"user_tz":-420,"elapsed":4,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["X_train = pd.read_csv(os.path.join(DIR_PATH, 'X_train_prep.csv')) \n","X_test = pd.read_csv(os.path.join(DIR_PATH, 'X_test_prep.csv'))\n","y_train = pd.read_csv(os.path.join(DIR_PATH, 'y_train_prep.csv'))\n","y_test = pd.read_csv(os.path.join(DIR_PATH, 'y_test_prep.csv'))"],"metadata":{"id":"87DftqOGbPID","executionInfo":{"status":"ok","timestamp":1658338028064,"user_tz":-420,"elapsed":2572,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["y_train = y_train.to_numpy().ravel()\n","y_test = y_test.to_numpy().ravel()"],"metadata":{"id":"JWaqlmuCbU5k","executionInfo":{"status":"ok","timestamp":1658338028065,"user_tz":-420,"elapsed":8,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["X_train.info()"],"metadata":{"id":"UYIjIidUQlF4","executionInfo":{"status":"ok","timestamp":1658338028065,"user_tz":-420,"elapsed":7,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3519949e-b5b9-46d6-9823-3e6a8e70179d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 14324 entries, 0 to 14323\n","Data columns (total 49 columns):\n"," #   Column              Non-Null Count  Dtype  \n","---  ------              --------------  -----  \n"," 0   Gender              14324 non-null  int64  \n"," 1   Own_car             14324 non-null  int64  \n"," 2   Own_property        14324 non-null  int64  \n"," 3   Work_phone          14324 non-null  int64  \n"," 4   Phone               14324 non-null  int64  \n"," 5   Email               14324 non-null  int64  \n"," 6   Unemployed          14324 non-null  int64  \n"," 7   Num_children        14324 non-null  float64\n"," 8   Num_family          14324 non-null  float64\n"," 9   Account_length      14324 non-null  float64\n"," 10  Total_income        14324 non-null  float64\n"," 11  Age                 14324 non-null  float64\n"," 12  Years_employed      14324 non-null  float64\n"," 13  Income_type_1       14324 non-null  int64  \n"," 14  Income_type_2       14324 non-null  int64  \n"," 15  Income_type_3       14324 non-null  int64  \n"," 16  Income_type_4       14324 non-null  int64  \n"," 17  Income_type_5       14324 non-null  int64  \n"," 18  Education_type      14324 non-null  float64\n"," 19  Family_status_1     14324 non-null  int64  \n"," 20  Family_status_2     14324 non-null  int64  \n"," 21  Family_status_3     14324 non-null  int64  \n"," 22  Family_status_4     14324 non-null  int64  \n"," 23  Family_status_5     14324 non-null  int64  \n"," 24  Housing_type_1      14324 non-null  int64  \n"," 25  Housing_type_2      14324 non-null  int64  \n"," 26  Housing_type_3      14324 non-null  int64  \n"," 27  Housing_type_4      14324 non-null  int64  \n"," 28  Housing_type_5      14324 non-null  int64  \n"," 29  Housing_type_6      14324 non-null  int64  \n"," 30  Occupation_type_1   14324 non-null  int64  \n"," 31  Occupation_type_2   14324 non-null  int64  \n"," 32  Occupation_type_3   14324 non-null  int64  \n"," 33  Occupation_type_4   14324 non-null  int64  \n"," 34  Occupation_type_5   14324 non-null  int64  \n"," 35  Occupation_type_6   14324 non-null  int64  \n"," 36  Occupation_type_7   14324 non-null  int64  \n"," 37  Occupation_type_8   14324 non-null  int64  \n"," 38  Occupation_type_9   14324 non-null  int64  \n"," 39  Occupation_type_10  14324 non-null  int64  \n"," 40  Occupation_type_11  14324 non-null  int64  \n"," 41  Occupation_type_12  14324 non-null  int64  \n"," 42  Occupation_type_13  14324 non-null  int64  \n"," 43  Occupation_type_14  14324 non-null  int64  \n"," 44  Occupation_type_15  14324 non-null  int64  \n"," 45  Occupation_type_16  14324 non-null  int64  \n"," 46  Occupation_type_17  14324 non-null  int64  \n"," 47  Occupation_type_18  14324 non-null  int64  \n"," 48  Occupation_type_19  14324 non-null  int64  \n","dtypes: float64(7), int64(42)\n","memory usage: 5.4 MB\n"]}]},{"cell_type":"markdown","source":["48/2 = 24\n","choose (32, 16)"],"metadata":{"id":"c9h9U9HTQvoW"}},{"cell_type":"markdown","source":["> NotImplementedError: Cannot convert a symbolic tf.Tensor (strided_slice_1:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n","  "],"metadata":{"id":"E592z7MUdE8C"}},{"cell_type":"code","source":["model = models.Sequential()\n","model.add(layers.Dense(\n","    49, input_shape=(49,), activation='relu'\n","))\n","model.add(layers.Dense(\n","    32, activation='relu'\n","))\n","model.add(layers.Dense(\n","    16, activation='relu'\n","))\n","model.add(layers.Dense(\n","    1, activation='sigmoid'\n","))\n","model.compile(\n","    loss='binary_crossentropy', optimizer='adam', metrics=[f2_score]\n",")\n","model.fit(X_train, y_train, epochs=20, batch_size=50)"],"metadata":{"id":"Z494xo-jPbQt","executionInfo":{"status":"error","timestamp":1658338031903,"user_tz":-420,"elapsed":3843,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}},"colab":{"base_uri":"https://localhost:8080/","height":762},"outputId":"b999fdc9-3f8f-4a1b-ce8d-2dfdbc08375d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n"]},{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-0b727af24623>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf2_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-6-c054e3d0ddae>\", line 1, in None  *\n        lambda y_test, y_pred: fbeta_score(y_test, y_pred, beta=2)\n    File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1259, in fbeta_score  *\n        _, _, f, _ = precision_recall_fscore_support(\n    File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1539, in precision_recall_fscore_support  *\n        labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n    File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1348, in _check_set_wise_labels  *\n        y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 84, in _check_targets  *\n        check_consistent_length(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 330, in check_consistent_length  *\n        uniques = np.unique(lengths)\n    File \"<__array_function__ internals>\", line 6, in unique  **\n        \n    File \"/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py\", line 270, in unique\n        ar = np.asanyarray(ar)\n\n    NotImplementedError: Cannot convert a symbolic Tensor (strided_slice_1:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"]}]},{"cell_type":"code","source":["! python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4t0w1lKWpqEw","executionInfo":{"status":"ok","timestamp":1658338078973,"user_tz":-420,"elapsed":532,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}},"outputId":"f7249f61-1639-454d-cd4e-55c10600f6f7"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.13\n"]}]},{"cell_type":"code","source":["pip show pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"On5wuRsDp1Gi","executionInfo":{"status":"ok","timestamp":1658338421494,"user_tz":-420,"elapsed":7170,"user":{"displayName":"Nhật Minh Hoàng Trần","userId":"17860532926939693584"}},"outputId":"b409a9f6-62af-4d4d-bcf6-9d108b23c866"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: pandas\n","Version: 1.3.5\n","Summary: Powerful data structures for data analysis, time series, and statistics\n","Home-page: https://pandas.pydata.org\n","Author: The Pandas Development Team\n","Author-email: pandas-dev@python.org\n","License: BSD-3-Clause\n","Location: /usr/local/lib/python3.7/dist-packages\n","Requires: numpy, pytz, python-dateutil\n","Required-by: xarray, vega-datasets, statsmodels, sklearn-pandas, seaborn, pymc3, plotnine, pandas-profiling, pandas-gbq, pandas-datareader, mlxtend, mizani, holoviews, gspread-dataframe, google-colab, fix-yahoo-finance, fbprophet, fastai, cufflinks, cmdstanpy, arviz, altair\n"]}]}]}